{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark DataFrame\n",
    "* Tabular data\n",
    "  * Rows \n",
    "  * Named Columns\n",
    "* Immutable \n",
    "* Distribute collection of data\n",
    "* Lazy\n",
    "* Can process structured and semi-structured data\n",
    "  * relational database\n",
    "  * csv\n",
    "  * json\n",
    "  * txt\n",
    "  * RDD\n",
    "  * dict\n",
    "  * list\n",
    "  * etc\n",
    "* Support SQL or expression methods\n",
    "  * SELECT * FROM RedWine\n",
    "  * red_wine_df.select()\n",
    "* Schema\n",
    "  * Information about\n",
    "    * column name\n",
    "    * data type\n",
    "    * empty values\n",
    "    * etc\n",
    "  * Help to optimize the queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=Create DataFrame, master=spark://192.168.2.102:7077) created by __init__ at /tmp/ipykernel_1898442/1114236199.py:9 ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1898442/1114236199.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# RDD\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmaster\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# DataFrame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/repos/PySpark/.venv/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[1;32m    142\u001b[0m                 \" is not allowed as it is a security risk.\")\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         \u001b[0mSparkContext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    145\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[0;32m~/repos/PySpark/.venv/lib/python3.8/site-packages/pyspark/context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[0;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m                     \u001b[0;31m# Raise error if there is already a running Spark context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m                     raise ValueError(\n\u001b[0m\u001b[1;32m    351\u001b[0m                         \u001b[0;34m\"Cannot run multiple SparkContexts at once; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m                         \u001b[0;34m\"existing SparkContext(app=%s, master=%s)\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=Create DataFrame, master=spark://192.168.2.102:7077) created by __init__ at /tmp/ipykernel_1898442/1114236199.py:9 "
     ]
    }
   ],
   "source": [
    "from pyspark import SparkContext \n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "master = 'spark://192.168.2.102:7077' # Connect to remote server\n",
    "appName = 'Create DataFrame'\n",
    "\n",
    "# RDD\n",
    "sc = SparkContext(master=master, appName=appName)\n",
    "\n",
    "# DataFrame\n",
    "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 12:18:26 WARN TaskSchedulerImpl: Initial job has not accepted any resources; check your cluster UI to ensure that workers are registered and have sufficient resources\n"
     ]
    }
   ],
   "source": [
    "df = spark.createDataFrame([\n",
    "    {'Id': 1, 'Value': 1},\n",
    "    {'Id': 1, 'Value': 2},\n",
    "    {'Id': 2, 'Value': 3},\n",
    "    {'Id': 2, 'Value': 4},\n",
    "])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Create DataFrame from RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[7.4, 0.7, 0.0, 5],\n",
       " [7.8, 0.88, 0.0, 5],\n",
       " [7.8, 0.76, 0.04, 5],\n",
       " [11.2, 0.28, 0.56, 6]]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_wine_rdd = sc.parallelize([\n",
    "    [7.4, 0.7, 0.0, 5],\n",
    "    [7.8, 0.88, 0.0, 5],\n",
    "    [7.8, 0.76, 0.04, 5],\n",
    "    [11.2, 0.28, 0.56, 6],\n",
    "])\n",
    "\n",
    "red_wine_rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+-------+\n",
      "|fixed acidity|volatile acidity|citric acid|quality|\n",
      "+-------------+----------------+-----------+-------+\n",
      "|          7.4|             0.7|        0.0|      5|\n",
      "|          7.8|            0.88|        0.0|      5|\n",
      "|          7.8|            0.76|       0.04|      5|\n",
      "|         11.2|            0.28|       0.56|      6|\n",
      "+-------------+----------------+-----------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "columns = ['fixed acidity', 'volatile acidity', 'citric acid', 'quality']\n",
    "\n",
    "red_wine_df = spark.createDataFrame(red_wine_rdd, schema=columns)\n",
    "\n",
    "red_wine_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fixed acidity', 'double'),\n",
       " ('volatile acidity', 'double'),\n",
       " ('citric acid', 'double'),\n",
       " ('quality', 'bigint')]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_wine_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|fixed acidity|volatile acidity|citric acid|residual sugar|chlorides|free sulfur dioxide|total sulfur dioxide|density|  pH|sulphates|alcohol|quality|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
      "|          7.8|            0.88|        0.0|           2.6|    0.098|               25.0|                67.0| 0.9968| 3.2|     0.68|    9.8|      5|\n",
      "|          7.8|            0.76|       0.04|           2.3|    0.092|               15.0|                54.0|  0.997|3.26|     0.65|    9.8|      5|\n",
      "|         11.2|            0.28|       0.56|           1.9|    0.075|               17.0|                60.0|  0.998|3.16|     0.58|    9.8|      6|\n",
      "|          7.4|             0.7|        0.0|           1.9|    0.076|               11.0|                34.0| 0.9978|3.51|     0.56|    9.4|      5|\n",
      "+-------------+----------------+-----------+--------------+---------+-------------------+--------------------+-------+----+---------+-------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "root_path = 'hdfs://192.168.2.102:9000/dataset/{filename}'\n",
    "\n",
    "# By Default inferSchema is False\n",
    "red_wine_df = spark.read.csv(root_path.format(filename='winequality-red.csv'), header=True, inferSchema=True)\n",
    "\n",
    "red_wine_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fixed acidity', 'double'),\n",
       " ('volatile acidity', 'double'),\n",
       " ('citric acid', 'double'),\n",
       " ('residual sugar', 'double'),\n",
       " ('chlorides', 'double'),\n",
       " ('free sulfur dioxide', 'double'),\n",
       " ('total sulfur dioxide', 'double'),\n",
       " ('density', 'double'),\n",
       " ('pH', 'double'),\n",
       " ('sulphates', 'double'),\n",
       " ('alcohol', 'double'),\n",
       " ('quality', 'int')]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_wine_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create DataFrame from txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|            value|\n",
      "+-----------------+\n",
      "|2021-12-01;1;20.0|\n",
      "|2021-12-02;1;20.2|\n",
      "|    2021-12-03;1;|\n",
      "|2021-12-04;1;20.3|\n",
      "|2021-12-05;1;20.5|\n",
      "+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist = spark.read.text('data/beer_temp_hist.txt')\n",
    "temp_hist.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2021-12-01;1;20.0',\n",
       " '2021-12-02;1;20.2',\n",
       " '2021-12-03;1;',\n",
       " '2021-12-04;1;20.3',\n",
       " '2021-12-05;1;20.5']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd = sc.textFile('data/beer_temp_hist.txt')\n",
    "rdd.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|      Date|BeerId|Temp|\n",
      "+----------+------+----+\n",
      "|2021-12-01|     1|20.0|\n",
      "|2021-12-02|     1|20.2|\n",
      "|2021-12-03|     1|    |\n",
      "|2021-12-04|     1|20.3|\n",
      "|2021-12-05|     1|20.5|\n",
      "|2021-12-01|     2|16.5|\n",
      "|2021-12-02|     2|16.4|\n",
      "|2021-12-03|     2|16.5|\n",
      "|2021-12-04|     2|    |\n",
      "|2021-12-05|     2|16.8|\n",
      "|2021-12-05|     2|16.7|\n",
      "|2021-12-01|     3|18.3|\n",
      "|2021-12-02|     3|18.4|\n",
      "|2021-12-03|     3|    |\n",
      "|2021-12-01|     4|18.2|\n",
      "+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "splitted_rows = rdd.map(lambda row: row.split(';'))\n",
    "temp_hist = spark.createDataFrame(splitted_rows, schema=['Date', 'BeerId', 'Temp'])\n",
    "temp_hist.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'string'), ('BeerId', 'string'), ('Temp', 'string')]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_hist.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Infer data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('fixed acidity', 'double'),\n",
       " ('volatile acidity', 'double'),\n",
       " ('citric acid', 'double'),\n",
       " ('residual sugar', 'double'),\n",
       " ('chlorides', 'double'),\n",
       " ('free sulfur dioxide', 'double'),\n",
       " ('total sulfur dioxide', 'double'),\n",
       " ('density', 'double'),\n",
       " ('pH', 'double'),\n",
       " ('sulphates', 'double'),\n",
       " ('alcohol', 'double'),\n",
       " ('quality', 'int')]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "red_wine_df = spark.read.csv(root_path.format(filename='winequality-red.csv'), header=True, inferSchema=True)\n",
    "red_wine_df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType, StringType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_hist_schema = StructType([\n",
    "    StructField('Date', DateType()),\n",
    "    StructField('BeerId', IntegerType()),\n",
    "    StructField('Temp', FloatType()),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|      Date|BeerId|Temp|\n",
      "+----------+------+----+\n",
      "|2021-12-01|     1|20.0|\n",
      "|2021-12-02|     1|20.2|\n",
      "|2021-12-03|     1|null|\n",
      "|2021-12-04|     1|20.3|\n",
      "|2021-12-05|     1|20.5|\n",
      "|2021-12-01|     2|16.5|\n",
      "|2021-12-02|     2|16.4|\n",
      "|2021-12-03|     2|16.5|\n",
      "|2021-12-04|     2|null|\n",
      "|2021-12-05|     2|16.8|\n",
      "|2021-12-05|     2|16.7|\n",
      "|2021-12-01|     3|18.3|\n",
      "|2021-12-02|     3|18.4|\n",
      "|2021-12-03|     3|null|\n",
      "|2021-12-01|     4|18.2|\n",
      "+----------+------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "temp_hist_df = spark.read.csv('data/beer_temp_hist.txt', sep=';', schema=temp_hist_schema)\n",
    "temp_hist_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Date', 'date'), ('BeerId', 'int'), ('Temp', 'float')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp_hist_df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date: date (nullable = true)\n",
      " |-- BeerId: integer (nullable = true)\n",
      " |-- Temp: float (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+-----------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|ElapsedDays|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|          4|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|          3|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|          2|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|          1|\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|          0|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|          4|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|          4|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|          3|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|          2|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|          1|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|          0|\n",
      "|  3| 2021-12-01|    null|2021-12-03|null|              null|          2|\n",
      "|  3| 2021-12-01|    null|2021-12-02|18.4|  65.1199993133545|          1|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3| 64.93999862670898|          0|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|          0|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "beer_df = beer_df.withColumn('ElapsedDays', F.datediff(beer_df.Date, beer_df.InitialDate))\n",
    "\n",
    "beer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|          4|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|          3|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|          2|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|          1|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|          3|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|          1|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|          0|    true|\n",
      "|  3| 2021-12-01|    null|2021-12-03|null|              null|          2|   false|\n",
      "|  3| 2021-12-01|    null|2021-12-02|18.4|  65.1199993133545|          1|   false|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3| 64.93999862670898|          0|    true|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|          0|    true|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df = beer_df.withColumn('FirstDay', beer_df.ElapsedDays == 0)\n",
    "beer_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "#### Select and filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bear_id = beer_df.select('Id')\n",
    "bear_id.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|          3|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|          1|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|          0|    true|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_2 = beer_df.filter(beer_df.Id == 2)\n",
    "beer_2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fill Missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|          4|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|          3|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|          2|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|          1|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|          3|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|          1|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|          0|    true|\n",
      "|  3| 2021-12-01|    null|2021-12-03|null|              null|          2|   false|\n",
      "|  3| 2021-12-01|    null|2021-12-02|18.4|  65.1199993133545|          1|   false|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3| 64.93999862670898|          0|    true|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|          0|    true|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|          4|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|          3|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|          2|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|          1|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|          3|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|          1|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|          0|    true|\n",
      "|  3| 2021-12-01| Unknown|2021-12-03|null|              null|          2|   false|\n",
      "|  3| 2021-12-01| Unknown|2021-12-02|18.4|  65.1199993133545|          1|   false|\n",
      "|  3| 2021-12-01| Unknown|2021-12-01|18.3| 64.93999862670898|          0|    true|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|          0|    true|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df = beer_df.fillna('Unknown', subset=['Type'])\n",
    "beer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18.233333269755047\n",
      "+---+-----------+--------+----------+---------+------------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|        C|                 F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+---------+------------------+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|     20.5|              68.9|          4|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|     20.3| 68.53999862670898|          3|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|18.233334|              null|          2|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|     20.2| 68.36000137329103|          1|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|     20.0|              68.0|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|     16.7|62.060001373291016|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|     16.8| 62.23999862670898|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|18.233334|              null|          3|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|     16.5|              61.7|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|     16.4| 61.51999931335449|          1|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|     16.5|              61.7|          0|    true|\n",
      "|  3| 2021-12-01| Unknown|2021-12-03|18.233334|              null|          2|   false|\n",
      "|  3| 2021-12-01| Unknown|2021-12-02|     18.4|  65.1199993133545|          1|   false|\n",
      "|  3| 2021-12-01| Unknown|2021-12-01|     18.3| 64.93999862670898|          0|    true|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|     18.2| 64.76000137329102|          0|    true|\n",
      "+---+-----------+--------+----------+---------+------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mean = beer_df.agg(F.mean('C')).collect()[0].asDict()['avg(C)']\n",
    "print(mean)\n",
    "beer_df.fillna(mean, 'C').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 47:=================================================>    (185 + 5) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+\n",
      "| Id|      Date|                 F|         F[Filled]|\n",
      "+---+----------+------------------+------------------+\n",
      "|  1|2021-12-01|              68.0|              68.0|\n",
      "|  1|2021-12-02| 68.36000137329103| 68.36000137329103|\n",
      "|  1|2021-12-03|              null| 68.36000137329103|\n",
      "|  1|2021-12-04| 68.53999862670898| 68.53999862670898|\n",
      "|  1|2021-12-05|              68.9|              68.9|\n",
      "|  2|2021-12-01|              61.7|              61.7|\n",
      "|  2|2021-12-02| 61.51999931335449| 61.51999931335449|\n",
      "|  2|2021-12-03|              61.7|              61.7|\n",
      "|  2|2021-12-04|              null|              61.7|\n",
      "|  2|2021-12-05|62.060001373291016|62.060001373291016|\n",
      "|  2|2021-12-05| 62.23999862670898| 62.23999862670898|\n",
      "|  3|2021-12-01| 64.93999862670898| 64.93999862670898|\n",
      "|  3|2021-12-02|  65.1199993133545|  65.1199993133545|\n",
      "|  3|2021-12-03|              null|  65.1199993133545|\n",
      "|  4|2021-12-01| 64.76000137329102| 64.76000137329102|\n",
      "+---+----------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.window import Window\n",
    "\n",
    "window = Window\\\n",
    "    .partitionBy('Id')\\\n",
    "    .orderBy('Date')\\\n",
    "    .rowsBetween(Window.unboundedPreceding, Window.currentRow)\n",
    "\n",
    "filled_cols = F.last(beer_df.F, ignorenulls=True).over(window)\n",
    "beer_df = beer_df.withColumn('F[Filled]', filled_cols)\n",
    "\n",
    "beer_df.select(['Id', 'Date', 'F', 'F[Filled]']).orderBy(['Id', 'Date']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 50:=======================================>              (146 + 4) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+------------------+------------------+\n",
      "| Id|      Date|                 F|         F[Filled]|\n",
      "+---+----------+------------------+------------------+\n",
      "|  1|2021-12-01|              68.0|              68.0|\n",
      "|  1|2021-12-02| 68.36000137329103| 68.36000137329103|\n",
      "|  1|2021-12-03|              null| 68.53999862670898|\n",
      "|  1|2021-12-04| 68.53999862670898| 68.53999862670898|\n",
      "|  1|2021-12-05|              68.9|              68.9|\n",
      "|  2|2021-12-01|              61.7|              61.7|\n",
      "|  2|2021-12-02| 61.51999931335449| 61.51999931335449|\n",
      "|  2|2021-12-03|              61.7|              61.7|\n",
      "|  2|2021-12-04|              null|62.060001373291016|\n",
      "|  2|2021-12-05|62.060001373291016|62.060001373291016|\n",
      "|  2|2021-12-05| 62.23999862670898| 62.23999862670898|\n",
      "|  3|2021-12-01| 64.93999862670898| 64.93999862670898|\n",
      "|  3|2021-12-02|  65.1199993133545|  65.1199993133545|\n",
      "|  3|2021-12-03|              null|              null|\n",
      "|  4|2021-12-01| 64.76000137329102| 64.76000137329102|\n",
      "+---+----------+------------------+------------------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "window = Window\\\n",
    "    .partitionBy('Id')\\\n",
    "    .orderBy('Date')\\\n",
    "    .rowsBetween(Window.currentRow, Window.unboundedFollowing)\n",
    "\n",
    "filled_cols = F.first(beer_df.F, ignorenulls=True).over(window)\n",
    "beer_df = beer_df.withColumn('F[Filled]', filled_cols)\n",
    "\n",
    "beer_df.select(['Id', 'Date', 'F', 'F[Filled]']).orderBy(['Id', 'Date']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "\n",
    "for i in range(1, 3):\n",
    "    new_col = f'F_-{i}'\n",
    "    cols.append(new_col)\n",
    "\n",
    "    window = Window\\\n",
    "        .partitionBy('Id')\\\n",
    "        .orderBy('Date')\\\n",
    "        .rowsBetween(Window.currentRow -i, Window.currentRow -i)\n",
    "\n",
    "    lag_col = F.first(beer_df['F[Filled]'], ignorenulls=True).over(window)\n",
    "    beer_df = beer_df.withColumn(new_col, lag_col)\n",
    "\n",
    "beer_df.select(['Id', 'Date', 'F[Filled]', *cols]).orderBy(['Id', 'Date']).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GroupBy Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df_grouped = beer_df.groupBy('Id')\n",
    "print(beer_df_grouped.count().show())\n",
    "print(beer_df_grouped.min('C').show())\n",
    "print(beer_df_grouped.max('C').show())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df.groupBy('Id').agg(\n",
    "    F.count('Id'), \n",
    "    F.min('C'), \n",
    "    F.max('C')\n",
    ").show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Other Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df.orderBy('C', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df.orderBy(['Id', 'C'], ascending=[False, True]).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df.dropDuplicates(['Id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_schema = StructType([\n",
    "    StructField('Id', IntegerType()),\n",
    "    StructField('InitialDate', DateType()),\n",
    "    StructField('Type', StringType()),\n",
    "])\n",
    "\n",
    "temp_hist_schema = StructType([\n",
    "    StructField('Date', DateType()),\n",
    "    StructField('Id', IntegerType()),\n",
    "    StructField('Temp', FloatType()),\n",
    "])\n",
    "\n",
    "beer_df = spark.read.csv('data/beer.csv', schema=beer_schema, header=True)\n",
    "beer_df.createOrReplaceTempView('Beer')\n",
    "\n",
    "\n",
    "temp_hist_df = spark.read.csv('data/beer_temp_hist.txt', sep=';', schema=temp_hist_schema)\n",
    "temp_hist_df.createOrReplaceTempView('TempHist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_hist_df = temp_hist_df.join(beer_df, on='Id')\n",
    "print(beer_hist_df.show(5))\n",
    "\n",
    "beer_hist_df = spark.sql(\"\"\"\n",
    "SELECT \n",
    "    TempHist.Id,\n",
    "    TempHist.Date,\n",
    "    TempHist.Temp,\n",
    "    Beer.InitialDate,\n",
    "    Beer.Type\n",
    "FROM TempHist\n",
    "INNER JOIN Beer ON \n",
    "    Beer.Id = TempHist.Id\n",
    "\"\"\")\n",
    "\n",
    "print(beer_hist_df.show(5))\n",
    "\n",
    "beer_hist_df.createOrReplaceTempView('BeerHist')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_df = spark.sql('SELECT * FROM BeerHist')\n",
    "beer_df.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bear_id = beer_hist_df.select('Id')\n",
    "print(bear_id.show(2))\n",
    "\n",
    "ear_id = spark.sql('SELECT Id FROM BeerHist')\n",
    "print(bear_id.show(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beer_2 = beer_hist_df.filter(beer_hist_df.Id == 2).orderBy('Temp', ascending=False)\n",
    "print(beer_2.show())\n",
    "\n",
    "beer_2 = spark.sql('SELECT * FROM BeerHist WHERE Id = 2 order by Temp desc')\n",
    "print(beer_2.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    beer_hist_df.groupBy('Id').agg(\n",
    "        F.min('Temp'), F.max('Temp')\n",
    "    ).show()\n",
    ")\n",
    "\n",
    "print(\n",
    "    spark.sql('SELECT Id, MIN(Temp), max(Temp) FROM BeerHist GROUP BY id').show()\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
