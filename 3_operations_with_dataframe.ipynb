{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations and Actions with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21/12/22 15:00:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "21/12/22 15:00:08 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n",
      "21/12/22 15:00:08 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType, StringType\n",
    "\n",
    "spark = SparkSession.builder\\\n",
    "    .master('spark://192.168.2.102:7077')\\\n",
    "    .appName('Operations and Actions with DataFrame')\\\n",
    "    .config(\"spark.cores.max\", \"1\")\\\n",
    "    .config(\"spark.executor.memory\", \"1g\")\\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|      Date|BeerId|Temp|\n",
      "+----------+------+----+\n",
      "|2021-12-01|     1|20.0|\n",
      "|2021-12-02|     1|20.2|\n",
      "|2021-12-03|     1|null|\n",
      "|2021-12-04|     1|20.3|\n",
      "|2021-12-05|     1|20.5|\n",
      "|2021-12-01|     2|16.5|\n",
      "|2021-12-02|     2|16.4|\n",
      "|2021-12-03|     2|16.5|\n",
      "|2021-12-04|     2|null|\n",
      "|2021-12-05|     2|16.8|\n",
      "|2021-12-05|     2|16.7|\n",
      "|2021-12-01|     3|18.3|\n",
      "|2021-12-02|     3|18.4|\n",
      "|2021-12-03|     3|null|\n",
      "|2021-12-01|     4|18.2|\n",
      "+----------+------+----+\n",
      "\n",
      "+---+-----------+--------+\n",
      "| Id|InitialDate|    Type|\n",
      "+---+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|\n",
      "|  2| 2021-12-01|Pale Ale|\n",
      "|  3| 2021-12-01|    null|\n",
      "|  4| 2021-12-01|     Ipa|\n",
      "+---+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_schema = StructType([\n",
    "    StructField('Date', DateType()),\n",
    "    StructField('BeerId', IntegerType()),\n",
    "    StructField('Temp', FloatType()),\n",
    "])\n",
    "\n",
    "beer_schema = StructType([\n",
    "    StructField('Id', IntegerType()),\n",
    "    StructField('InitialDate', DateType()),\n",
    "    StructField('Type', StringType()),\n",
    "])\n",
    " \n",
    "\n",
    "temp_hist_df = spark.read.csv('data/beer_temp_hist.txt', sep=';', schema=temp_hist_schema)\n",
    "beer_description_df = spark.read.csv('data/beer.csv', schema=beer_schema, header=True)\n",
    "\n",
    "temp_hist_df.show()\n",
    "beer_description_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename column\n",
    "\n",
    "You can rename a columns with 'withColumnRenamed('ColumnName', 'NewColumnName')' method When the columns is renamed, a new DataFrame is created. Remember, spark is lazy, and will only compute the new DataFrame if an action is called, in this case, 'show'. \n",
    "\n",
    "You can also chain operations like in the second example. In the second we will atribute the new DataFrame into the same variable, so the garbage collector will delete the older temp_hist_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|      Date|BeerId|   C|\n",
      "+----------+------+----+\n",
      "|2021-12-01|     1|20.0|\n",
      "|2021-12-02|     1|20.2|\n",
      "|2021-12-03|     1|null|\n",
      "|2021-12-04|     1|20.3|\n",
      "|2021-12-05|     1|20.5|\n",
      "+----------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_renamend_df = temp_hist_df.withColumnRenamed('Temp', 'C')\n",
    "temp_hist_renamend_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "|      Date| Id|   C|\n",
      "+----------+---+----+\n",
      "|2021-12-01|  1|20.0|\n",
      "|2021-12-02|  1|20.2|\n",
      "|2021-12-03|  1|null|\n",
      "|2021-12-04|  1|20.3|\n",
      "|2021-12-05|  1|20.5|\n",
      "+----------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_df = temp_hist_df\\\n",
    "    .withColumnRenamed('Temp', 'C')\\\n",
    "    .withColumnRenamed('BeerId', 'Id')\n",
    "\n",
    "temp_hist_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new columns\n",
    "\n",
    "Create columns is important to create new features, and we can create columns doing operations and adding to the DataFrame with 'withColumn' method. In the example below, qe convert Celsius to Fahrenheit.\n",
    "\n",
    "$ F = C * 1.8 + 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----------------+\n",
      "|      Date| Id|   C|                F|\n",
      "+----------+---+----+-----------------+\n",
      "|2021-12-01|  1|20.0|             68.0|\n",
      "|2021-12-02|  1|20.2|68.36000137329103|\n",
      "|2021-12-03|  1|null|             null|\n",
      "|2021-12-04|  1|20.3|68.53999862670898|\n",
      "|2021-12-05|  1|20.5|             68.9|\n",
      "+----------+---+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert C to F\n",
    "F = temp_hist_df.C * 1.8 + 32\n",
    "\n",
    "# Create a new DataFrame with F column\n",
    "temp_hist_df = temp_hist_df.withColumn('F', F)\n",
    "\n",
    "temp_hist_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop existent column\n",
    "\n",
    "You can discard columns that you will not use by calling 'drop' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "|      Date| Id|   C|\n",
      "+----------+---+----+\n",
      "|2021-12-01|  1|20.0|\n",
      "|2021-12-02|  1|20.2|\n",
      "|2021-12-03|  1|null|\n",
      "|2021-12-04|  1|20.3|\n",
      "|2021-12-05|  1|20.5|\n",
      "+----------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_without_f_df = temp_hist_df.drop('F')\n",
    "temp_hist_without_f_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_ids_df = temp_hist_df\\\n",
    "    .drop('F')\\\n",
    "    .drop('C')\\\n",
    "    .drop('Date')\n",
    "\n",
    "temp_hist_ids_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge DataFrames\n",
    "\n",
    "An import step to create features is to cross the date between two DataFrames.\n",
    "* inner\n",
    "* left (leftouter, left_outer)\n",
    "* right (rightouter, right_outer)\n",
    "* cross\n",
    "* full (fullouter, full_outer)\n",
    "* semi (leftsemi, left_semi)\n",
    "* anti (leftanti, left_anti)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = spark.createDataFrame([\n",
    "    [1, 'EX_1'],\n",
    "    [2, 'EX_2'],\n",
    "    [3, 'EX_3'],\n",
    "    [7, 'EX_7'],\n",
    "], schema=['Id', 'Description'])\n",
    "\n",
    "df2 = spark.createDataFrame([\n",
    "    [1, 1],\n",
    "    [2, 2],\n",
    "    [2, 2],\n",
    "    [3, 3],\n",
    "    [3, 3],\n",
    "    [4, 4],\n",
    "    [5, 5],\n",
    "], schema=['Id', 'Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:===========================================>            (58 + 4) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+\n",
      "| Id|Description|Value|\n",
      "+---+-----------+-----+\n",
      "|  1|       EX_1|    1|\n",
      "|  3|       EX_3|    3|\n",
      "|  3|       EX_3|    3|\n",
      "|  2|       EX_2|    2|\n",
      "|  2|       EX_2|    2|\n",
      "+---+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on='Id', how='inner').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 36:===================================>                    (48 + 4) / 75]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+\n",
      "| Id|Description|Value|\n",
      "+---+-----------+-----+\n",
      "|  7|       EX_7| null|\n",
      "|  1|       EX_1|    1|\n",
      "|  3|       EX_3|    3|\n",
      "|  3|       EX_3|    3|\n",
      "|  2|       EX_2|    2|\n",
      "|  2|       EX_2|    2|\n",
      "+---+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on='Id', how='left').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+\n",
      "| Id|Description|Value|\n",
      "+---+-----------+-----+\n",
      "|  5|       null|    5|\n",
      "|  1|       EX_1|    1|\n",
      "|  3|       EX_3|    3|\n",
      "|  3|       EX_3|    3|\n",
      "|  2|       EX_2|    2|\n",
      "|  2|       EX_2|    2|\n",
      "|  4|       null|    4|\n",
      "+---+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df1.join(df2, on='Id', how='right').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+---+-----+\n",
      "| Id|Description| Id|Value|\n",
      "+---+-----------+---+-----+\n",
      "|  1|       EX_1|  1|    1|\n",
      "|  1|       EX_1|  2|    2|\n",
      "|  1|       EX_1|  2|    2|\n",
      "|  1|       EX_1|  3|    3|\n",
      "|  1|       EX_1|  3|    3|\n",
      "|  1|       EX_1|  4|    4|\n",
      "|  1|       EX_1|  5|    5|\n",
      "|  2|       EX_2|  1|    1|\n",
      "|  2|       EX_2|  2|    2|\n",
      "|  2|       EX_2|  2|    2|\n",
      "|  2|       EX_2|  3|    3|\n",
      "|  2|       EX_2|  3|    3|\n",
      "|  2|       EX_2|  4|    4|\n",
      "|  2|       EX_2|  5|    5|\n",
      "|  3|       EX_3|  1|    1|\n",
      "|  3|       EX_3|  2|    2|\n",
      "|  3|       EX_3|  2|    2|\n",
      "|  3|       EX_3|  3|    3|\n",
      "|  3|       EX_3|  3|    3|\n",
      "|  3|       EX_3|  4|    4|\n",
      "+---+-----------+---+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# For each row of df1, we will have a row of df2\n",
    "df1.join(df2, how='cross').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+\n",
      "| Id|Description|Value|\n",
      "+---+-----------+-----+\n",
      "|  7|       EX_7| null|\n",
      "|  5|       null|    5|\n",
      "|  1|       EX_1|    1|\n",
      "|  3|       EX_3|    3|\n",
      "|  3|       EX_3|    3|\n",
      "|  2|       EX_2|    2|\n",
      "|  2|       EX_2|    2|\n",
      "|  4|       null|    4|\n",
      "+---+-----------+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Return all rows that match in both DataFrames\n",
    "df1.join(df2, how='full', on='Id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| Id|Description|\n",
      "+---+-----------+\n",
      "|  1|       EX_1|\n",
      "|  3|       EX_3|\n",
      "|  2|       EX_2|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Return columns from left DataFrame that matches columns with right DataFrame\n",
    "df1.join(df2, how='semi', on='Id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+\n",
      "| Id|Description|\n",
      "+---+-----------+\n",
      "|  7|       EX_7|\n",
      "+---+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Return columns from left DataFrame that don't matches columns with right DataFrame\n",
    "df1.join(df2, how='anti', on='Id').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|\n",
      "+---+-----------+--------+----------+----+------------------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3| 64.93999862670898|\n",
      "|  3| 2021-12-01|    null|2021-12-02|18.4|  65.1199993133545|\n",
      "|  3| 2021-12-01|    null|2021-12-03|null|              null|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|\n",
      "+---+-----------+--------+----------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df = beer_description_df.join(temp_hist_df, on='Id', how='inner')\n",
    "beer_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PySpark functions\n",
    "\n",
    "pyspark.sql.functions is a collection of built-in functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+----------+----+-----------------+-----------+\n",
      "| Id|InitialDate| Type|      Date|   C|                F|ElapsedDays|\n",
      "+---+-----------+-----+----------+----+-----------------+-----------+\n",
      "|  1| 2021-12-01|Laget|2021-12-01|20.0|             68.0|          0|\n",
      "|  1| 2021-12-01|Laget|2021-12-02|20.2|68.36000137329103|          1|\n",
      "|  1| 2021-12-01|Laget|2021-12-03|null|             null|          2|\n",
      "|  1| 2021-12-01|Laget|2021-12-04|20.3|68.53999862670898|          3|\n",
      "|  1| 2021-12-01|Laget|2021-12-05|20.5|             68.9|          4|\n",
      "+---+-----------+-----+----------+----+-----------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "elapsed_days = F.datediff(beer_df.Date, beer_df.InitialDate)\n",
    "beer_df = beer_df.withColumn('ElapsedDays', elapsed_days)\n",
    "beer_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+-----+----------+----+-----------------+-----------+--------+\n",
      "| Id|InitialDate| Type|      Date|   C|                F|ElapsedDays|FirstDay|\n",
      "+---+-----------+-----+----------+----+-----------------+-----------+--------+\n",
      "|  1| 2021-12-01|Laget|2021-12-01|20.0|             68.0|          0|    true|\n",
      "|  1| 2021-12-01|Laget|2021-12-02|20.2|68.36000137329103|          1|   false|\n",
      "|  1| 2021-12-01|Laget|2021-12-03|null|             null|          2|   false|\n",
      "|  1| 2021-12-01|Laget|2021-12-04|20.3|68.53999862670898|          3|   false|\n",
      "|  1| 2021-12-01|Laget|2021-12-05|20.5|             68.9|          4|   false|\n",
      "+---+-----------+-----+----------+----+-----------------+-----------+--------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df = beer_df.withColumn('FirstDay', beer_df.ElapsedDays == 0)\n",
    "beer_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|          4|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|          3|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|          1|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|          0|    true|\n",
      "|  3| 2021-12-01|    null|2021-12-02|18.4|  65.1199993133545|          1|   false|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3| 64.93999862670898|          0|    true|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|          1|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|          3|   false|\n",
      "|  3| 2021-12-01|    null|2021-12-03|null|              null|          2|   false|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df.orderBy('C', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|          0|    true|\n",
      "|  3| 2021-12-01|    null|2021-12-03|null|              null|          2|   false|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3| 64.93999862670898|          0|    true|\n",
      "|  3| 2021-12-01|    null|2021-12-02|18.4|  65.1199993133545|          1|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|          3|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|          1|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|          2|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|          4|   false|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|          4|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|          2|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|          0|    true|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|          1|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|          3|   false|\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|          4|   false|\n",
      "+---+-----------+--------+----------+----+------------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df.orderBy(['Id', 'C'], ascending=[False, True]).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Duplicated Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+-----------------+-----------+--------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                F|ElapsedDays|FirstDay|\n",
      "+---+-----------+--------+----------+----+-----------------+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|             68.0|          0|    true|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3|64.93999862670898|          0|    true|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2|64.76000137329102|          0|    true|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|             61.7|          0|    true|\n",
      "+---+-----------+--------+----------+----+-----------------+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df.dropDuplicates(['Id']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42197daa2389db0e10ddee9d5ff70ea816516a63390dbff66f18d3f0c6dfb379"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
