{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Operations and Actions with DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField\n",
    "from pyspark.sql.types import IntegerType, FloatType, DateType, StringType\n",
    "\n",
    "master = 'spark://192.168.2.102:7077' # Connect to remote server\n",
    "appName = 'Operations and Actions with DataFrame'\n",
    "\n",
    "spark = SparkSession.builder.appName(appName).master(master).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|      Date|BeerId|Temp|\n",
      "+----------+------+----+\n",
      "|2021-12-01|     1|20.0|\n",
      "|2021-12-02|     1|20.2|\n",
      "|2021-12-03|     1|null|\n",
      "|2021-12-04|     1|20.3|\n",
      "|2021-12-05|     1|20.5|\n",
      "|2021-12-01|     2|16.5|\n",
      "|2021-12-02|     2|16.4|\n",
      "|2021-12-03|     2|16.5|\n",
      "|2021-12-04|     2|null|\n",
      "|2021-12-05|     2|16.8|\n",
      "|2021-12-05|     2|16.7|\n",
      "|2021-12-01|     3|18.3|\n",
      "|2021-12-02|     3|18.4|\n",
      "|2021-12-03|     3|null|\n",
      "|2021-12-01|     4|18.2|\n",
      "+----------+------+----+\n",
      "\n",
      "+---+-----------+--------+\n",
      "| Id|InitialDate|    Type|\n",
      "+---+-----------+--------+\n",
      "|  1| 2021-12-01|   Laget|\n",
      "|  2| 2021-12-01|Pale Ale|\n",
      "|  3| 2021-12-01|    null|\n",
      "|  4| 2021-12-01|     Ipa|\n",
      "+---+-----------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_schema = StructType([\n",
    "    StructField('Date', DateType()),\n",
    "    StructField('BeerId', IntegerType()),\n",
    "    StructField('Temp', FloatType()),\n",
    "])\n",
    "\n",
    "beer_schema = StructType([\n",
    "    StructField('Id', IntegerType()),\n",
    "    StructField('InitialDate', DateType()),\n",
    "    StructField('Type', StringType()),\n",
    "])\n",
    " \n",
    "\n",
    "temp_hist_df = spark.read.csv('data/beer_temp_hist.txt', sep=';', schema=temp_hist_schema)\n",
    "beer_description_df = spark.read.csv('data/beer.csv', schema=beer_schema, header=True)\n",
    "\n",
    "temp_hist_df.show()\n",
    "beer_description_df.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rename column\n",
    "\n",
    "You can rename a columns with 'withColumnRenamed('ColumnName', 'NewColumnName')' method When the columns is renamed, a new DataFrame is created. Remember, spark is lazy, and will only compute the new DataFrame if an action is called, in this case, 'show'. \n",
    "\n",
    "You can also chain operations like in the second example. In the second we will atribute the new DataFrame into the same variable, so the garbage collector will delete the older temp_hist_df."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+------+----+\n",
      "|      Date|BeerId|   C|\n",
      "+----------+------+----+\n",
      "|2021-12-01|     1|20.0|\n",
      "|2021-12-02|     1|20.2|\n",
      "|2021-12-03|     1|null|\n",
      "|2021-12-04|     1|20.3|\n",
      "|2021-12-05|     1|20.5|\n",
      "+----------+------+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_renamend_df = temp_hist_df.withColumnRenamed('Temp', 'C')\n",
    "temp_hist_renamend_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "|      Date| Id|   C|\n",
      "+----------+---+----+\n",
      "|2021-12-01|  1|20.0|\n",
      "|2021-12-02|  1|20.2|\n",
      "|2021-12-03|  1|null|\n",
      "|2021-12-04|  1|20.3|\n",
      "|2021-12-05|  1|20.5|\n",
      "+----------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_df = temp_hist_df\\\n",
    "    .withColumnRenamed('Temp', 'C')\\\n",
    "    .withColumnRenamed('BeerId', 'Id')\n",
    "\n",
    "temp_hist_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new columns\n",
    "\n",
    "Create columns is important to create new features, and we can create columns doing operations and adding to the DataFrame with 'withColumn' method. In the example below, qe convert Celsius to Fahrenheit.\n",
    "\n",
    "$ F = C * 1.8 + 32$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+-----------------+\n",
      "|      Date| Id|   C|                F|\n",
      "+----------+---+----+-----------------+\n",
      "|2021-12-01|  1|20.0|             68.0|\n",
      "|2021-12-02|  1|20.2|68.36000137329103|\n",
      "|2021-12-03|  1|null|             null|\n",
      "|2021-12-04|  1|20.3|68.53999862670898|\n",
      "|2021-12-05|  1|20.5|             68.9|\n",
      "+----------+---+----+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Convert C to F\n",
    "F = temp_hist_df.C * 1.8 + 32\n",
    "\n",
    "# Create a new DataFrame with F column\n",
    "temp_hist_df = temp_hist_df.withColumn('F', F)\n",
    "\n",
    "temp_hist_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop existent column\n",
    "\n",
    "You can discard columns that you will not use by calling 'drop' method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+----+\n",
      "|      Date| Id|   C|\n",
      "+----------+---+----+\n",
      "|2021-12-01|  1|20.0|\n",
      "|2021-12-02|  1|20.2|\n",
      "|2021-12-03|  1|null|\n",
      "|2021-12-04|  1|20.3|\n",
      "|2021-12-05|  1|20.5|\n",
      "+----------+---+----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_without_f_df = temp_hist_df.drop('F')\n",
    "temp_hist_without_f_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| Id|\n",
      "+---+\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "|  1|\n",
      "+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "temp_hist_ids_df = temp_hist_df\\\n",
    "    .drop('F')\\\n",
    "    .drop('C')\\\n",
    "    .drop('Date')\n",
    "\n",
    "temp_hist_ids_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge DataFrames\n",
    "\n",
    "An import step to create features is to cross the date between two DataFrames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+-----------+--------+----------+----+------------------+\n",
      "| Id|InitialDate|    Type|      Date|   C|                 F|\n",
      "+---+-----------+--------+----------+----+------------------+\n",
      "|  1| 2021-12-01|   Laget|2021-12-01|20.0|              68.0|\n",
      "|  1| 2021-12-01|   Laget|2021-12-02|20.2| 68.36000137329103|\n",
      "|  1| 2021-12-01|   Laget|2021-12-03|null|              null|\n",
      "|  1| 2021-12-01|   Laget|2021-12-04|20.3| 68.53999862670898|\n",
      "|  1| 2021-12-01|   Laget|2021-12-05|20.5|              68.9|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-01|16.5|              61.7|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-02|16.4| 61.51999931335449|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-03|16.5|              61.7|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-04|null|              null|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.8| 62.23999862670898|\n",
      "|  2| 2021-12-01|Pale Ale|2021-12-05|16.7|62.060001373291016|\n",
      "|  3| 2021-12-01|    null|2021-12-01|18.3| 64.93999862670898|\n",
      "|  3| 2021-12-01|    null|2021-12-02|18.4|  65.1199993133545|\n",
      "|  3| 2021-12-01|    null|2021-12-03|null|              null|\n",
      "|  4| 2021-12-01|     Ipa|2021-12-01|18.2| 64.76000137329102|\n",
      "+---+-----------+--------+----------+----+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "beer_df = beer_description_df.join(temp_hist_df, on='Id', how='inner')\n",
    "beer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "42197daa2389db0e10ddee9d5ff70ea816516a63390dbff66f18d3f0c6dfb379"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
